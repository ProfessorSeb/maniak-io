---\ntitle: \"Your First AI Route: Connecting to OpenAI with AgentGateway (Open Source)\"\ndate: 2026-02-11\ndescription: \"Step-by-step guide to connecting open source AgentGateway to OpenAI API with cost tracking, monitoring, security best practices, and production-ready configurations.\"\n---\n\n# Your First AI Route: Connecting to OpenAI with AgentGateway (Open Source)\n\n## Introduction\nThis is a how-to guide to setup AgentGateway and get your first AI route working with OpenAI. We'll walk through the complete setup from scratch - creating a Kubernetes cluster, installing AgentGateway, and connecting it to OpenAI's API.\n\n## What You'll Learn\n\n- Create a Kubernetes cluster and install AgentGateway\n- Set up secure OpenAI API key storage\n- Configure AgentGateway to route to OpenAI\n- Test chat completions, embeddings, and model listings\n- Monitor real AI requests and track costs\n- Troubleshoot common issues\n\n## Prerequisites\n\n- Docker installed and running\n- kubectl CLI tool\n- Helm 3.x installed  \n- Valid OpenAI API Key with credits (get from [OpenAI Platform](https://platform.openai.com))\n- Basic understanding of Kubernetes and OpenAI API structure\n\n## Overview of AgentGateway\n\nAgentGateway is an open-source AI gateway designed specifically for Kubernetes environments. It acts as a unified entry point for routing requests to various AI providers, including OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, Google Gemini, Vertex AI, and OpenAI-compatible endpoints like Ollama. Built on the Kubernetes Gateway API, AgentGateway provides advanced features such as:\n\n- **Provider Configuration**: Easy setup for multiple AI backends with model overrides and custom endpoints.\n- **Security Policies**: Prompt guards for PII protection, jailbreak prevention, and credential leak detection.\n- **Rate Limiting**: Request-based and token-based limits to control usage and costs.\n- **Prompt Enrichment**: Automatic injection of system prompts for consistent AI behavior.\n- **Model Failover**: Reliability patterns for switching between providers on failures.\n- **Observability**: Built-in tracing, metrics, and logging with OpenTelemetry support.\n\nAgentGateway comes in open-source (agentgateway.dev) and enterprise editions, with the latter offering additional features like advanced authentication and UI management.\n\n### Why Run AgentGateway in Kubernetes?\n\nDeploying AgentGateway in Kubernetes leverages the platform's strengths for production AI infrastructure:\n\n- **Scalability**: Horizontally scale gateway instances to handle high traffic volumes.\n- **High Availability**: Use Kubernetes replicas and health checks for zero-downtime operations.\n- **Declarative Management**: Define configurations as YAML manifests using CRDs, enabling GitOps workflows.\n- **Integration**: Seamlessly works with Kubernetes networking, secrets management, and observability tools.\n- **Security**: Benefit from pod security standards, network policies, and role-based access control.\n- **Efficiency**: Run alongside your applications in the same cluster, reducing latency and simplifying operations.\n\nThis Kubernetes-native approach makes AgentGateway ideal for teams building reliable, observable AI systems at scale.\n\n---\n\n## Step 1: Environment Setup\n\nIn this step, we'll create a local Kubernetes cluster using kind (Kubernetes in Docker) and install AgentGateway. This gives us a complete testing environment that mirrors production setups but runs entirely on your local machine.\n\n### Install Kind\n\nKind creates Kubernetes clusters using Docker containers as nodes. This is perfect for development and testing because it's lightweight, fast to spin up, and doesn't require cloud resources.\n```bash\n# On macOS \nbrew install kind\n\n# On Linux\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64\nchmod +x ./kind && sudo mv ./kind /usr/local/bin/kind\n```\n\n### Create Kind Cluster\n\nThis creates a single-node Kubernetes cluster that will host our AgentGateway installation. The cluster provides the foundation for all the networking, security, and routing capabilities we'll configure.\n\n```bash\n# Create the cluster\nkind create cluster --name agentgateway\n\n# Verify cluster is ready\nkubectl get nodes\n```\n\n### Install AgentGateway\n\nAgentGateway installation happens in three phases: First we install the Kubernetes Gateway API (the standard for ingress traffic), then AgentGateway's custom resources, and finally the control plane that manages everything. This separation allows for better modularity and easier upgrades.\n```bash\n# 1. Install Gateway API CRDs (version 1.4.0)\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.0/standard-install.yaml\n\n# 2. Install AgentGateway CRDs\nhelm upgrade -i --create-namespace \\\n  --namespace agentgateway-system \\\n  --version v2.2.0 agentgateway-crds \\\n  oci://ghcr.io/kgateway-dev/charts/agentgateway-crds\n\n# 3. Install AgentGateway control plane\nhelm upgrade -i -n agentgateway-system agentgateway \\\n  oci://ghcr.io/kgateway-dev/charts/agentgateway \\\n  --version v2.2.0\n\n# 4. Verify installation\nkubectl get pods -n agentgateway-system\n```\n\n---\n\n## Step 2: OpenAI API Key Setup\n\nSecurity is paramount when working with AI services. Instead of embedding API keys directly in configurations, we'll use Kubernetes secrets to store credentials securely. This approach ensures keys are encrypted at rest and can be rotated without changing application code.\n\n### Get Your OpenAI API Key\n\nOpenAI uses API keys for authentication and billing. Each key is tied to your account and usage limits, making it essential to secure them properly.\n1. Visit [OpenAI Platform](https://platform.openai.com)\n2. Navigate to API Keys section and create a new key\n3. Set usage limits to control costs\n4. Copy your API key securely\n\n### Test Your API Key\n\nBefore integrating with AgentGateway, we'll verify the API key works directly with OpenAI's API. This eliminates the key as a potential issue if something goes wrong later in the setup.\n```bash\n# Set your OpenAI API key (replace with your actual key)\nexport OPENAI_API_KEY=\"sk-your-openai-api-key-here\"\n\n# Test the key directly\ncurl -s \"https://api.openai.com/v1/models\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  | jq '.data[0:3] | .[].id'\n```\n\n### Create Kubernetes Secret\n\nKubernetes secrets provide a secure way to store sensitive data like API keys. We format the key as a complete Authorization header (`Bearer sk-...`) so AgentGateway can use it directly without modification. The `--dry-run=client -o yaml | kubectl apply -f -` pattern ensures the secret is created safely even if it already exists.\n```bash\n# Create secret with proper authorization header format\nkubectl create secret generic openai-secret \\\n  -n agentgateway-system \\\n  --from-literal=\"Authorization=Bearer $OPENAI_API_KEY\" \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n# Verify secret creation\nkubectl get secret openai-secret -n agentgateway-system\n```\n\n---\n\n## Step 3: Configure AgentGateway\n\nNow we'll configure the core components that make AI routing work. AgentGateway follows the Kubernetes Gateway API pattern with three main resources: Gateway (the entry point), Backends (destination services), and HTTPRoutes (traffic routing rules). This declarative approach makes configurations version-controllable and environment-portable.\n\n### Create Gateway Resource\n\nThe Gateway resource defines the entry point for all incoming traffic. It specifies which ports to listen on, what protocols to accept, and which namespaces can create routes through it. Think of it as the front door to your AI services.\n```bash\nkubectl apply -f- <<'EOF'\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: agentgateway-proxy\n  namespace: agentgateway-system\nspec:\n  gatewayClassName: agentgateway\n  listeners:\n  - protocol: HTTP\n    port: 8080\n    name: http\n    allowedRoutes:\n      namespaces:\n        from: All\nEOF\n```\n\n### Create OpenAI Backend\n\nAgentgatewayBackend resources define how to connect to AI services. The `ai.provider.openai` section tells AgentGateway this is an AI service that expects OpenAI-compatible requests. The authentication policy references our secret, and the timeout ensures long-running AI requests don't hang indefinitely.\n```bash\nkubectl apply -f- <<'EOF'\napiVersion: agentgateway.dev/v1alpha1\nkind: AgentgatewayBackend\nmetadata:\n  name: openai-backend\n  namespace: agentgateway-system\nspec:\n  ai:\n    provider:\n      openai:\n        model: gpt-4o-mini\n  policies:\n    auth:\n      secretRef:\n        name: openai-secret\n    http:\n      requestTimeout: 120s\nEOF\n```\n\n### Create HTTP Routes\n\nHTTPRoutes define how incoming requests map to backend services. We need two different backend types because OpenAI's API has two distinct patterns: AI endpoints that process JSON payloads (like chat completions) and simple REST endpoints for metadata (like listing models).\n\n**Why two backends?** AgentGateway's AI-aware backends expect JSON payloads and provide token counting, cost tracking, and observability. But simple GET endpoints like `/models` don't fit this pattern, so we use a static HTTP backend that passes requests through unchanged.\n\nFirst, create a backend for non-AI endpoints (like models list):\n\n```bash\nkubectl apply -f- <<'EOF'\napiVersion: agentgateway.dev/v1alpha1\nkind: AgentgatewayBackend\nmetadata:\n  name: openai-models-backend\n  namespace: agentgateway-system\nspec:\n  policies:\n    auth:\n      secretRef:\n        name: openai-secret\n    http:\n      requestTimeout: 30s\n  static:\n    host: api.openai.com\n    port: 443\nEOF\n```\n\nThen create the HTTP routes:\n```bash\nkubectl apply -f- <<'EOF'\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: openai-chat\n  namespace: agentgateway-system\nspec:\n  parentRefs:\n  - name: agentgateway-proxy\n    namespace: agentgateway-system\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /openai/chat/completions\n    backendRefs:\n    - name: openai-backend\n      group: agentgateway.dev\n      kind: AgentgatewayBackend\n    timeouts:\n      request: \"120s\"\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: /v1/chat/completions\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: openai-models\n  namespace: agentgateway-system\nspec:\n  parentRefs:\n  - name: agentgateway-proxy\n    namespace: agentgateway-system\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /openai/models\n    backendRefs:\n    - name: openai-models-backend\n      group: agentgateway.dev\n      kind: AgentgatewayBackend\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: /v1/models\nEOF\n```\n\n### Verify Configuration\n\nBefore testing, we'll check that all our resources are properly created and accepted by the AgentGateway controller. The `Accepted` status indicates that configurations are valid and the controller can proceed with implementation.\n```bash\n# Check both backends\nkubectl get agentgatewaybackend -n agentgateway-system\n\n# Check routes\nkubectl get httproute -n agentgateway-system\n\n# Check Gateway\nkubectl get gateway agentgateway-proxy -n agentgateway-system\n```\n\n---\n\n## Step 4: Testing Your Setup\n\nWith our configuration complete, it's time to test the AI routing. Since we're using a local kind cluster, we'll use port-forwarding to access the Gateway service. In production, this would be handled by a LoadBalancer or Ingress controller.\n\n### Setup Port-Forward\n\nPort-forwarding creates a tunnel from your local machine to the AgentGateway service inside the Kubernetes cluster. This lets us test the setup without exposing services publicly.\n```bash\n# Port-forward AgentGateway service in background\nkubectl port-forward -n agentgateway-system svc/agentgateway-proxy 8080:8080 &\n\n# Store the process ID to kill it later\nPORTFORWARD_PID=$!\necho \"Port-forward running as PID: $PORTFORWARD_PID\"\n\n# Set gateway endpoint\nexport GATEWAY_IP=\"localhost\"\nexport GATEWAY_PORT=\"8080\"\n```\n\n### Test Chat Completions\n\nThis is where the magic happens! Our request travels through AgentGateway, gets authenticated using our secret, routed to OpenAI's API, and returns with a complete AI response. Notice how the response includes token usage information that AgentGateway automatically captures for cost tracking and observability.\n```bash\n# Test basic chat completion\ncurl -i \"$GATEWAY_IP:$GATEWAY_PORT/openai/chat/completions\" \\\n  -H \"content-type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What are the key benefits of using an AI Gateway?\"\n      }\n    ],\n    \"max_tokens\": 100\n  }'\n```\n\n**Expected Response:**\n```json\n{\n  \"id\": \"chatcmpl-abc123def456\",\n  \"object\": \"chat.completion\",\n  \"created\": 1701234567,\n  \"model\": \"gpt-4o-mini-2024-07-18\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"An AI Gateway provides unified access to multiple AI providers, centralized security and authentication, comprehensive observability and cost tracking, rate limiting and quotas, and improved reliability through failover and retry mechanisms.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 15,\n    \"completion_tokens\": 35,\n    \"total_tokens\": 50\n  }\n}\n```\n\n### Test Different Models\n\nAgentGateway allows you to easily switch between different OpenAI models by simply changing the `model` parameter. The backend automatically routes to the appropriate model while maintaining consistent authentication and observability.\n```bash\n# Test with GPT-4o\ncurl -s \"$GATEWAY_IP:$GATEWAY_PORT/openai/chat/completions\" \\\n  -H \"content-type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Explain AgentGateway in one sentence.\"\n      }\n    ],\n    \"max_tokens\": 50\n  }' | jq '.choices[0].message.content'\n```\n\n### Test Models List\n\nThe models endpoint demonstrates our dual-backend approach in action. This simple GET request uses our static backend to retrieve the list of available models directly from OpenAI, bypassing AI-specific processing since it's just metadata.\n```bash\n# List available models (check raw response first)\ncurl -i \"$GATEWAY_IP:$GATEWAY_PORT/openai/models\"\n\n# If successful, filter for GPT models\ncurl -s \"$GATEWAY_IP:$GATEWAY_PORT/openai/models\" | jq -r '.data[]? | select(.id | contains(\"gpt\")) | .id'\n```\n\n---\n\n## Step 5: Monitoring and Observability\n\nOne of AgentGateway's key advantages is comprehensive observability out of the box. Every AI request generates structured logs with token usage, timing, and cost information. This visibility is crucial for production AI systems where costs can escalate quickly and performance directly impacts user experience.\n```bash\n# Check what logs look like first\nkubectl logs deploy/agentgateway -n agentgateway-system --tail=5\n\n# View structured logs with AI context (filter JSON lines only)\nkubectl logs deploy/agentgateway -n agentgateway-system --tail=50 | \\\n  grep '^{' | \\\n  jq 'select(.gen_ai?) | {\n    timestamp: .timestamp,\n    model: .gen_ai.request.model,\n    prompt_tokens: .gen_ai.usage.prompt_tokens,\n    completion_tokens: .gen_ai.usage.completion_tokens,\n    duration: .duration\n  }'\n```\n\n### Monitor Costs\n\nAI services bill based on token usage, making cost monitoring essential. This script demonstrates how to extract token usage from AgentGateway logs and calculate estimated costs using current OpenAI pricing. In production, you'd integrate this with alerting systems to prevent budget overruns.\n```bash\n# Create cost calculation script\ncat <<'EOF' > calculate-costs.sh\n#!/bin/bash\n\necho \"Analyzing recent token usage...\"\n\nkubectl logs deploy/agentgateway -n agentgateway-system --tail=50 | \\\n  grep '^{' | \\\n  jq -r 'select(.gen_ai.usage?) | [\n    .timestamp,\n    .gen_ai.request.model,\n    .gen_ai.usage.prompt_tokens,\n    .gen_ai.usage.completion_tokens,\n    .gen_ai.usage.total_tokens\n  ] | @csv' | \\\n  awk -F',' '\nBEGIN {\n  print \"Model,Input Tokens,Output Tokens,Total Tokens,Estimated Cost\"\n  total_cost = 0\n}\n{\n  model = $2\n  input = $3\n  output = $4\n  gsub(/\"/, \"\", model)\n  \n  cost = 0\n  if (model ~ /gpt-4o-mini/) {\n    cost = (input * 0.000150 / 1000) + (output * 0.000600 / 1000)\n  } else if (model ~ /gpt-4o/) {\n    cost = (input * 0.0025 / 1000) + (output * 0.0100 / 1000)\n  }\n  \n  total_cost += cost\n  printf \"%s,%d,%d,%d,$%.6f\\n\", model, input, output, input+output, cost\n}\nEND {\n  printf \"\\nTotal estimated cost: $%.6f\\n\", total_cost\n}'\nEOF\n\nchmod +x calculate-costs.sh\n./calculate-costs.sh\n```\n\n---\n\n## Troubleshooting\n\nWhen working with distributed systems like Kubernetes and external APIs, issues can arise at multiple layers. This section covers the most common problems you might encounter and how to systematically diagnose them. The key is to test each layer independently: network connectivity, authentication, resource configuration, and API compatibility.\n\n### Common Issues\n\n**1. Service Not Found Error:**\nThis usually means the service name doesn't match what was actually created during installation. Different AgentGateway versions or installation methods may create services with different names.\n```bash\n# Check what services exist\nkubectl get svc -n agentgateway-system\n\n# If agentgateway-proxy doesn't exist, use the correct service name\nkubectl get svc -n agentgateway-system | grep -i gateway\n```\n\n**2. Authentication Errors (401):**\nAuthentication failures typically indicate either an invalid API key or incorrect secret formatting. Always test the key directly with OpenAI before troubleshooting AgentGateway.\n```bash\n# Verify secret exists\nkubectl get secret openai-secret -n agentgateway-system -o yaml\n\n# Test API key directly\ncurl -s \"https://api.openai.com/v1/models\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" | jq '.data[0].id'\n```\n\n**3. Routes Not Working:**\nRoute issues often stem from mismatched resource names or namespaces. The Gateway, HTTPRoute, and Backend must all reference each other correctly for traffic to flow.\n```bash\n# Check backend status\nkubectl describe agentgatewaybackend openai-backend -n agentgateway-system\n\n# Check route status\nkubectl describe httproute openai-chat -n agentgateway-system\n\n# Check Gateway status\nkubectl describe gateway agentgateway-proxy -n agentgateway-system\n```\n\n**4. Port-Forward Issues:**\nPort conflicts are common on development machines. If port 8080 is busy, either stop the conflicting service or use a different port.\n```bash\n# Check if port 8080 is in use\nlsof -i :8080\n\n# Try a different port\nkubectl port-forward -n agentgateway-system svc/agentgateway-proxy 8081:8080 &\nexport GATEWAY_PORT=\"8081\"\n```\n\n### Debug Commands\n```bash\n# View all AgentGateway resources\nkubectl get agentgatewaybackend,gateway,httproute -n agentgateway-system\n\n# Check pod logs for errors\nkubectl logs deploy/agentgateway -n agentgateway-system --tail=20\n\n# Test connectivity from inside cluster\nkubectl exec -n agentgateway-system deploy/agentgateway -- \\\n  curl -v https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n```\n\n---\n\n## Cleanup\n\nWhen you're done experimenting, it's important to clean up resources to free up system resources and avoid any potential costs. The cleanup process should happen in reverse order: stop network connections first, then remove application resources, and finally remove infrastructure.\n\n### Stop Port-Forward\n```bash\n# Kill the port-forward process\nkill $PORTFORWARD_PID\n```\n\n### Remove Resources (Optional)\n\nThis removes all the AgentGateway configuration we created, but leaves the AgentGateway installation intact for future experiments. Remove resources in dependency order: routes first (they reference backends), then backends, then the Gateway.\n```bash\n# Remove all OpenAI configuration\nkubectl delete httproute openai-chat openai-models -n agentgateway-system\nkubectl delete agentgatewaybackend openai-backend openai-models-backend -n agentgateway-system\nkubectl delete gateway agentgateway-proxy -n agentgateway-system\nkubectl delete secret openai-secret -n agentgateway-system\n```\n\n### Remove Kind Cluster (Optional)\n\nThis completely removes the Kubernetes cluster and all associated resources. Only do this if you're completely done with the tutorial, as you'll need to recreate everything from Step 1 to run it again.\n```bash\n# Delete the entire cluster\nkind delete cluster --name agentgateway\n```\n\n---\n\n## Next Steps\n\nNow that you have a working AI gateway, you can build on this foundation to create production-ready AI infrastructure:\n\n- **Add more providers** - Configure Anthropic, AWS Bedrock, or Azure OpenAI for multi-provider setups and failover scenarios\n- **Implement security** - Add rate limiting, authentication, and guardrails to protect against abuse and unexpected costs\n- **Set up monitoring** - Configure Grafana dashboards and alerting to track performance, costs, and usage patterns across teams\n- **Explore advanced routing** - Implement path-based, header-based, and weighted routing to direct different types of requests to optimal models\n\n## Key Takeaways\n\nThis tutorial demonstrates several important concepts for production AI systems:\n\n- **AgentGateway provides a unified interface** to AI providers with minimal overhead, making it easy to switch providers or implement failover\n- **Proper secret management** is essential for production deployments - never embed API keys in code or configuration files\n- **Built-in observability** gives immediate insights into costs and performance without requiring additional tooling or instrumentation\n- **The Gateway API pattern** makes routing configuration declarative and portable across different Kubernetes environments\n- **Dual backend types** (AI-aware vs static HTTP) allow you to handle both complex AI workloads and simple metadata requests efficiently\n- **Kind clusters** are perfect for local development and testing, providing a production-like environment without cloud costs\n\nYour AgentGateway is now successfully routing requests to OpenAI with enterprise-grade security, observability, and cost control! You've built a foundation that can scale from development to production while maintaining visibility and control over your AI infrastructure. ðŸŽ¯